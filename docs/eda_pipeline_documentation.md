# Anxietyâ€‘Model Project â€“ EDA Pipeline Documentation

*Last updated: 11â€¯Junâ€¯2025*

---

## 1. Purpose

This document records every dataâ€‘preparation step you have completed inside the `` folder so far.  It explains **what each script does, what files it produces, and how these pieces fit together** so that anyone (including futureâ€‘you) can reproduce or extend the pipeline.

---

## 2. Current Directory Layout

```
projectâ€‘root/
â”œâ”€â”€ data/                     # Raw DAICâ€‘WOZ ZIPs + metadata â†“
â”‚Â Â  â””â”€â”€ â€¦
â”œâ”€â”€ EDA/
â”‚   â”œâ”€â”€ clean_text/           # Cleaned transcript .txt files (per participant)
â”‚   â”‚Â Â  â”œâ”€â”€ 301_TRANSCRIPT_clean.txt
â”‚   â”‚Â Â  â””â”€â”€ â€¦
â”‚   â”œâ”€â”€ clean_transcripts.py  # StepÂ 2.1 â€“ transcript cleaning
â”‚   â”œâ”€â”€ text_feature_extraction.py   # StepÂ 2.2A/B â€“ LIWC + lexical
â”‚   â”œâ”€â”€ extract_nlp_features.py      # wrapper / helper (tokenization etc.)
â”‚   â”œâ”€â”€ filter_emotion_features.py   # optional extra emotion filtering
â”‚   â”œâ”€â”€ sentence_embedding_extraction.py  # StepÂ 2.2C â€“ utterance embeddings â†’ CSV
â”‚   â”œâ”€â”€ aggregate_embeddings.py      # Averages embeddings per participant
â”‚   â”œâ”€â”€ features.csv                 # LIWC + lexical features (participantâ€‘level)
â”‚   â”œâ”€â”€ nlp_features.csv             # Same as above but interim
â”‚   â”œâ”€â”€ emotion_features.csv         # Emotion counts (optional)
â”‚   â”œâ”€â”€ sentence_embeddings.csv      # Utteranceâ€‘level embeddings (384â€‘d)
â”‚   â”œâ”€â”€ participant_embeddings.csv   # 384â€‘d mean vector per participant
â”‚   â””â”€â”€ final_features_with_labels.csv   # <â€” to be produced in next step
â”œâ”€â”€ venv/ (or conda env)
â””â”€â”€ requirements.txt
```

---

## 3. Environment Setup

```bash
# create & activate venv (example)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# install core libraries
pip install -r requirements.txt
#   key packages: pandas, nltk, empath, sentenceâ€‘transformers, scikitâ€‘learn
```

---

## 4. Stepâ€‘byâ€‘Step Pipeline

| Step        | Script / Action                                                | Input                                                                              | Key Operations                                                                                                  | Output                                      |
| ----------- | -------------------------------------------------------------- | ---------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| **2.1**     | `clean_transcripts.py`                                         | Raw `*_TRANSCRIPT.txt`                                                             | â€¢ Remove timestamps, filler charsâ€¢ Lowerâ€‘case, strip speaker tagsâ€¢ Save perâ€‘participant clean file              | `EDA/clean_text/XXX_TRANSCRIPT_clean.txt`   |
| **2.2â€¯A/B** | `text_feature_extraction.py` (calls `extract_nlp_features.py`) | Clean text files                                                                   | â€¢ LIWC category counts via Empathâ€¢ Lexical diversity (TTR, MTLD, etc.)â€¢ Hedge frequency, cognitiveâ€‘load proxies | `features.csv` (participantâ€‘level)          |
| (opt.)      | `filter_emotion_features.py`                                   | `features.csv`                                                                     | â€¢ Keep emotionâ€‘relevant LIWC rows only                                                                          | `emotion_features.csv`                      |
| **2.2â€¯C**   | `sentence_embedding_extraction.py`                             | Clean text files                                                                   | â€¢ Load `allâ€‘MiniLMâ€‘L6â€‘v2` modelâ€¢ Encode each utterance (384 dims)                                               | `sentence_embeddings.csv` (utteranceâ€‘level) |
|             | `aggregate_embeddings.py`                                      | `sentence_embeddings.csv`                                                          | â€¢ Meanâ€‘pool embeddings per participant                                                                          | `participant_embeddings.csv`                |
| **2.3**     | *(to build)* merge script                                      | `features.csv`, `participant_embeddings.csv`, `emotion_features.csv`, `labels.csv` | â€¢ Innerâ€‘join on `participant_id`                                                                                | `final_features_with_labels.csv`            |
| **2.4**     | model training notebook / script                               | `final_features_with_labels.csv`                                                   | â€¢ Logistic Regression, Random Forest, SVMâ€¢ Train/val/test split (stratified)                                    | Saved model + metrics                       |
| **2.5**     | anxietyâ€‘risk clf (future)                                      | Depressed subset from 2.4                                                          | â€¢ Predict `anxiety` within depressed                                                                            | Metrics + model                             |

---

## 5. Detailed Script Descriptions

### 5.1 `clean_transcripts.py`

- **Goal:** standardise raw DAIC transcripts for NLP.
- **Highlights:**
  - Uses regex to drop `[laugh]`, timestamps, XML tags.
  - Tokenises into sentences with NLTK (for later embedding granularity).

### 5.2 `text_feature_extraction.py` & `extract_nlp_features.py`

- **Goal:** produce participantâ€‘level linguistic features inspired by prior depressionâ€‘detection papers.
- **Key Features Generated:**
  - **LIWCâ€‘style counts** (emotion, cognitive process, social words) via Empath categories.
  - **Lexical diversity:** Typeâ€‘Token Ratio (TTR), Hapax Legomena %, MTLD.
  - **Hedging & cognitiveâ€‘load cues:** frequency of "I guess", "maybe", pronoun shifts, average sentence length.

### 5.3 `sentence_embedding_extraction.py`

- **Goal:** get semantic representation of every utterance.
- **Model:** `allâ€‘MiniLMâ€‘L6â€‘v2` (Sentenceâ€‘Transformers; 384â€‘d; 90Â M params).
- **Process:** loops through each cleaned file, encodes lines, appends to list, saves `sentence_embeddings.csv`.

### 5.4 `aggregate_embeddings.py`

- **Goal:** convert utteranceâ€‘level embeddings to a single participant vector.
- **Pooling:** simple mean across utterances (empirically solid baseline; other options = CLS token, maxâ€‘pool).

### 5.5 Feature Merge Script (TBD)

Will:

```python
merged = (pd.read_csv('features.csv')
            .merge(pd.read_csv('participant_embeddings.csv'), on='participant_id')
            .merge(pd.read_csv('labels.csv'), on='participant_id'))
merged.to_csv('final_features_with_labels.csv', index=False)
```

---

## 6. Data Provenance Summary

| File                             | Rows                 | Level       | Notes                           |
| -------------------------------- | -------------------- | ----------- | ------------------------------- |
| `clean_text/*.txt`               | \~3â€“8â€¯k lines / file | Utterance   | Raw text cleaned                |
| `sentence_embeddings.csv`        | Î£ utterances         | Utterance   | 384â€‘d vectors + metadata        |
| `participant_embeddings.csv`     | \~150 participants   | Participant | Mean of embeddings              |
| `features.csv`                   | same                 | Participant | LIWC + lexical (\~50 dims)      |
| `emotion_features.csv`           | same                 | Participant | Optional subset of features     |
| `final_features_with_labels.csv` | same                 | Participant | (+) depression / anxiety labels |

---

## 7. Next Toâ€‘Dos

1. **Generate **`` using PHQâ€‘8 & GADâ€‘7 as discussed.
2. **Merge features** (Â§5.5) to create `final_features_with_labels.csv`.
3. **Train baseline classifiers** (LogReg / SVM / RandomForest); log metrics (accuracy, F1, AUC).
4. **Hyperâ€‘tune models**; feature importance analysis.
5. (**Optional**) Visualise embeddings with tâ€‘SNE/PCA for cluster sanityâ€‘check.
6. **Subâ€‘task 2.5:** Train anxietyâ€‘risk model on depressed subset.

---

## 8. Repro Tips

- Always fix random seeds (`numpy`, `torch`, `sklearn`) for reproducible splits.
- Cache heavy models in `~/.cache/` or project root `.models` to avoid reâ€‘download.
- Track package versions in `requirements.txt` after each new install.

---

## 9. Changelog

| Date        | Author           | Change                                                                       |
| ----------- | ---------------- | ---------------------------------------------------------------------------- |
| 10â€¯Junâ€¯2025 | Avreet + ChatGPT | Added LIWC + lexical extraction pipeline.                                    |
| 11â€¯Junâ€¯2025 | Avreet + ChatGPT | Added sentence embeddings + participant aggregation; initial docs generated. |

---

> **Legend:** Steps correspond to Avreetâ€™s original project outline (2.1Â â€“Â 2.5).  ğŸ

